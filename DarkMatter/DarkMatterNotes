An FPGA-based Data Acquisition System for Directional Dark Matter Detection

I. Introduction
	
	-Directional dark matter detection seeks to reconstruct the angular distribution of dark matter particles traveling through the 
	labratory.

	-This paper describes an FPGA-based digital BE system to handle a 16 Gbps data stream from 1000 different independent detector
	channels sampled at 1 MHz.

	-We are trying to detect the evidence of an interaction between a Weakly Interacting Massive Particle (WIMP) and a target nucleus.
	The interaction is an elastic collision that creates a recoiling nucleus. 

	-This collisions occur within a Time Projection Chamber (TPC) in which a nuclear recoil generates a track of ionization that is 
	drifted to a readout plane using a uniform magnetic field.


II. Detection Scheme and Physics Requirements

	-A WIMP may enter the TPC and interact with a fluorine nucleus of a SF6 molecule, generating a fluroine nucleus recoil.
	As the nucleus comes to rest, 	it generates a track of ionization electrons. The high electronegative SF6 gas will readily 
	attach the free elections, forming SF6-. This track of these ions then drifts under an applied electric field toward the 
	readout strips. There are 500 strips in the x and 500 strips in the y directions.

	-From the charge signal, we can reconstruct the recoil energy and track of the geometry of the event. The recoil energy is
	determined from the total deposited charge in the detector. The different in arrival time between the species at the readout
	plane is directly proportional to the total drift distance.

	-The event trigger is a critical piece of the Data Acquisition System (DAQ). Ideally, the event trigger would sense the total
	ionization of a recoil, not just a threshold voltage on a single channel. This requires that the trigger examine the integral
	of a waveform and incorporate the signal from several adjacent detector channels.

	-The DAQ must preserve 5 ms of pre-trigger and 15 ms of post-trigger data. The DAQ need only transfer data from a small subset
	of the channels to disk (e,g., the channels involved in the trigger, plus a few neighbors).


III. Data Acquisition System (DAQ)

	-The detector system consists of 3 major parts:
		1. TPC
		2. FE electronics for signal conditioning and digitization
		3. BE that handles the real time data stream, applies trigger conditions, and saves interesting events to a computer

	-FE consists of 8 FE boards that measure the charge signal on each of the 1000 strips. Each FE board provides analog signal
	conditioning and digitization, and outputs a single serialized stream of the digital data. The charge signal is first 
	conditioned by an custom analog ASIC and then digitized by a second custom ASIC. One FE board contains 8 analog and 8 ADC
	ASICS and a single Cyclone IV FPGA that handles the digital stream from the digitizer ASICs. 

	-The FE FPGA serializes the digital data for transmission on a single transceiver line. The result is a digital stream of 
	data from 125 detector channels. Each FE board operates independently, sending its data stream to a single BE at a data
	generation rate of 2 Gbps.

	-The BE must receive the digital data stream from 8 FE board (total data generation rate of 16 Gbps), store the data in a 
	circular buffer, apply triggering conditions, and save triggered data to a PC for off-line analysis. We use an Alter Cyclone V
	FPGA development board. The board has 8 transceiver ports, and 512 MB of DDR3 SDRAM with a x64 soft memory controller.

	-The FE-BE data link is a MGT, which is a serial link that provides low latency, high bandwidth, and low energy cost for
	FPGA-to-FPGA connections. Upon arrival at the BE, data goes through 3 stages:
		1. A pattern detector looks for triggering patterns.
		2. The serial data is aligned by time and channel number using time-stamp data attached to each data packet by the FE. 
		3. Triggered data is transferred from DRAM to a PC via UDP on Gigabit ethernet for off-line analysis.


IV. Implementation
	
	A. Front-end data packet generation

		-The ADC ASIC has 12-bit precision, while the transceiver encrypts and serializes 16-bit data each time. The extra 4 bits
		are used for in-packet indexing. Currently, we simply cycle the index for the 125-channel payload. In this way, data lost 
		during transmission can be easily detected. In addition to the payload, we add 3 extra 16-bit words for alignment across
		different packets: a starting word, and ending word, and a sampling-time stamp. So each packet is 2048 bits (256 bytes).

						|---- 16 bit ----|
							StartingWord
							 TimeStamp
						  Algn|SensorData
						  Algn|SensorData
						  Algn|SensorData
						  	EndingWord

	B. Transceiver Controller

		-To synchronize the 8 FE boards, a system reset signal is generated by a putton push on the BE. The signal is propogated
		to the 8 FE boards via GPIO in a daisy chain. On reset, the FE transceivers prepare the synchronization pattern and the 
		BE transceiver wait for the sync pattern to arrive. After the reset clears, all FE boards will repeatedly send out their
		alignment wordws for 1 ms. 

	C. Event Trigger

		-A BE event trigger is implemented on each of the 8 receiving transceiver output ports. The event trigger is implemented 
		as a FSM. When in reset mode, the trigger controller resets all control signals and loads the threshold value. 
		The controller stays in Initial State during that time. After the reset clears, the event trigger enters the Non-
		triggering state and processes each 16-bit data output from the transceiver. The detector first looks for a starting 
		word and then records the time-stamp from the following word. The 12 LSB of the payload are then sent to the comparator.

		-Upon triggering, the detector sets a global flag along the trigger time-stamp, which is checked by all channels every 
		cycle. In this way, all 1000 detector channels enter the triggered state at the same time. In the Triggering State, the 
		BE records 15,000 packets (15 ms at 1 MHz) before returning to the non-triggered state. The BE also sends the trigger 
		time-stamp to the DRAM controller to ensure preservation of 5 ms of pre-trigger data. 

		-We implement 2 triggering schemes: a per-channel voltage threshold trigger and a per-channel integral trigger:
			1. Threshold trigger, individual channel
				-In this scheme, the detector is triggered when the voltage rises above a fixed threshold on a single
				channel. These thresholds are pre-determined and stored in BRAM. For each FE, a 125 x 12 bit BRAM module 
				is implemented.	As the index for each datum is only 4-bit, we keep a 3-bit external counter in the 
				controller. Whenever an index overflow is detected, we increment the external counter. Hence, this 
				combined 7-bit index can address 125 data sampels inside a packet.

			2. Integral trigger, individual channel
				-A better trigger takes into account the integral of the waveform. We integrate over 5 samples, so we use
				125 12 x 5-bit shift registers to buffer the data. When a new sample arrives from the FE, the 60-bit word
				is updated by shifting the new data into the 12 MSB and discarding the 12 LSB. The 60-bit data is sent to
				a 4 stage pipeline for summation and comparison.

	D. DRAM controller

	-Buffering 5 ms of pre-trigger data for 1000 channels at 1 MHz requires (5000 packets * 256 bytes per packet * 8 channels)
	10MB. This is larger than the BRAM capacity, but the Cyclone V has 4 x16 SDRAM chips, which provides 512 MB of storage.
	The total data generation rate from the FE is 16 Gbps. At 300 MHz, the theoretical read and write bandwidth is 38.4 Gbps, 
	fast enough to write all received data directly to DRAM.
		-If 10MB is required for pre-trigger, means 30 MB is required for post-triggerm for a total of 40 MB per trigger.
		At 512 MB capacity, the DRAM can hold 12 triggering events.

	-In our design, we put the DRAM IP on the Avalon bus, which is an interface that simplifies the communication between 
	modules. The Avalon bus organizes the DRAM into 256-bit words. The 300 MHz DRAM working frequency is different from
	the transceiver block, so we implement asynchronous fifos between these two modules to deal with the metastability.
	The fifo write port has 16-bit width, while the ouput port is 256-bit, which matches the transceiver output port and
	DRAM interface, respectively. The buffer depth is set to 130 16-bit words to hold an entire packet from an FE board.

	-For easy data alignment, and to avoid data congestion, a double buffering mechanism is used. A one-bit global flag
	is implemented to bias the read and write fifos. Whenever the transceiver detects a FE packet header, the fifo
	selection flga is reversed to write to the other fifo, while the fifo that was previously being written to will
	be used as input for the DRAM.

	-Writes to DRAM are organized by sample time and aligned by the FE board number-of-origin. The 512 MB DRAM is 
	pre-allocated into 2^21 (2 million) slots, each slot holds 8 256-bit words to fit a single FE packet (2048 bits).
		-2^21 slots * 2^8 bytes per slot = 2^29 bytes = 512 MB

	-When an entire FE data packet is written into the related fifo and a ready signal is received, the DRAM controller enables
	the address generator to send a starting address based on the time-stamp and board number pointing to the pre-allocated
	space for that packet.

	-At a 16 Gbps data generation rate, 512 MB DRAM can only hold 256 ms of data. We must therefore resue the DRAM space,
	especially at low trigger rates. A replacement mechanism is implemented inside the DRAM address generator. From the 
	address generator point of view, the DRAM is divided up into units of 8 FE packets that hold data sampled at the
	same time, resulting in a total of 2^18 (256,000) units. A hash table is used to provide within-unit address offsets 
	based on the FE board number. Organizing data based on sample time both simplifies the replacement algorithm
	and minimized DRAM addressing latency.

	-A 1-bit address mask is assigned to each unit. If the data is determined to be useful by the event trigger, then 
	the related mask bit is set so all the data sampled at the same time will be kept. When the address generator reaches
	the end of the DRAM address space, the new incoming data is again assigned to address unit 0. Before that address
	is assigned, the mask bit is examined. If the bit is set, then the controller looks for the next unset mask bit and 
	uses the related address as the starting point for the following data sets. In this way, we keep the useful triggering
	data while fully reusing the non-triggering part,

	-We record 20ms of data each trigger (40 MB), which equates to 0.04 GB or 0.32 Gbit. Our DRAM is 512 MB (0.512 GB) or
	4 Gbit, so we can hold 12 triggering events.


//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
Emails with Professor Battat

June 6 (James):
	The following channel-by-channel asynchronous (local) triggering scheme is quite appealing to me.

	The guiding principles are:
		-trigger is spatially local, and is based on a single channel (either threshold or integral)
		-only triggered channels (plus neighbors) are read out
		-only triggered channels are held in triggered state

	Simplistic Implementation:
		-1000 detector channels
		-Each channel has:
		   - circular buffer in DRAM for data (e.g. 2000 samples long)
		   - trigger (threshold or integral) based only on its own data with "data ready" bit that gets set on trigger
		   - data index value that points to the start of the triggered waveform in the circular buffer.
		     (typically incremented by one when a new sample arrives, but gets latched on trigger)
		-On trigger:
		   - "data ready" bits for the triggered channel and its 2 neighbors are set to disable writing to the circular buffer for these channels
		   - channel numbers of triggered channel and neighbors are entered into a "look at me" FIFO
		   - data index value is latched (for triggered channel and neighbors)

		-A "stream to PC" module monitors the "LAM FIFO" looking for the next channel number to be read, gets the channel number
		and the latched data index, and streams that single channel of triggered waveform from DRAM to PC, along with timestamp.
		After streaming, the relevant "data ready" bit is reset (allowing new data to populate circular buffer)

		-In this scheme, BE doesn't attempt to construct events -- that's not its job. It just identifies and streams to disk 
		channel-by-channel waveforms, asynchronously.  The PC is in charge of reconstructing the event.

		-Main drawback of this scheme is the absence of an "integral across many channels" trigger.


June 11 (Ethan):
	-It's a good idea to have independent triggers for each one of the 1000 channels.

	-Currently, we already have 1000 independent checkers that monitor the data from all the channels in parallel. Need to remove
	global triggering coordinator that controls the entire system.

	-The tricky part:
		1. Need independent bit masks for each channel. The size of the bit mask is 2^14=16,384 bits. And times 1,000, 
		this will be a huge resource consumption. (previously we only need a single set).

		2. Now need separate address generators for each channel. The write address input port on the DRAM controller will take input from
		1000 sources, which requires a big mux, this may affect the timing of the design.


June 11 (James):
	-If each channel has a dedicated region of DRAM for its circular buffer (length = 1 waveform, e.g. 2000 samples),
	then can't there be a single bit to either allow or prevent the write?

	-Or are you describing a more capable system that can continue to accept data into the buffer for channel N while 
	waiting for triggered data from channel N to be read out?


June 11 (Ethan):
	-We have 512 MB DRAM on the board, each DRAM read & write word is 256-bits. Thus, for each channel, the available 
	slot is 512*1024*1024*8 / 1000 / 256 = 2^14 slots. We have a single bit mask for each one of the slot, thus we 
	need a total of 2^14 bit mask for each channel.

	-Currently the system will write all the data into the DRAM, as long as the reorder buffer has collected enough data samples 
	that can be combined into a 256-bit word (16*16-bit). But I'm thinking that the system should be capable of accepting new data 
	while the triggered data is waiting to be read out. That's why we need a bit-mask to keep track of which DRAM slots can be replaced. 

	-Are you suggesting that we should always stream out the data from DRAM to PC whenever a triggered event to captured? If that's 
	the case, we may not need to worry about the replacement.


June 11 (James):

	-Yes, in my straw man scheme, the data from a triggered channel is tagged for immediate readout. Well, since multiple 
	channels could trigger on the same clock cycle, I had proposed a FIFO that holds the channel numbers to be read out.


June 11 (Ethan):

	-In that case, we may not need the bit mask anymore. (Perhaps we still need it since it may not be able to stream 
	out immediately.) And we need a big MUX and an arbiter to control which channel data to stream off the board.
 

 June 28 (Ethan to me):
 	-We will implement streaming out data from DRAM to PC whenever a triggered event occurs. This is dependent upon
 	the streaming bandwidth to the PC (how was can we stream the data out?).
 	
 	-If we find that we stream out data faster than we can cycle through the DRAM, then a bitmask won't be necessary

 	-We are still going to do the independent channels method so that only triggered channels are streamed out

 	-So I guess that we are dividing the DRAM up into 1000 slots because we have 1000 channels

 	-We already have triggers that are based on a single channel
