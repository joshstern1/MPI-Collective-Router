//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
/*
comm_ptr contains:
	rank
	comm_size
	context_id
	intercommunicator vs. intracommunicator

*/
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
/*
COMMUNICATORS

	Communicators provides a separate communication space. It is possible to treat a subset of processes as a communication universe.

	A communicator is an object describing a group of processes. In many applications all processes work together closely coupled, 
	and the only communicator you need is MPI_COMM_WORLD , the group describing all processes that your job starts with.

	An important reason for using communicators is the development of software libraries. If the routines in a library use their 
	own communicator (even if it is a duplicate of the `outside' communicator), there will never be a confusion between message tags inside and outside the library.

	There are three predefined communicators:

	MPI_COMM_WORLD
	comprises all processes that were started together by mpirun (or some related program).

	MPI_COMM_SELF
	is the communicator that contains only the current process.

	MPI_COMM_NULL
	is the invalid communicator. Routines that construct communicators can give this as result if an error occurs

SUBCOMMUNICATORS

	In many scenarios you divide a large job over all the available processors. It makes sense to divide your processors into subgroups accordingly.
	As long as you only do sends and receives, this division works fine. However, if one group of processes needs to perform a collective operation, 
	you don't want the other groups involved in this. Thus, you really want all the groups to be really distinct from each other.

	In order to make such subsets of processes, MPI has the mechanism of taking a subset of MPI_COMM_WORLD and turning that subset into a new communicator.
	By making a communicator that contains a subset of all available processes, you can do a collective on that subset.

	Communicator constructors are collective routines, meaning they must be called by all processes in the group associated with the comm

DUPLICATING COMMUNICATORS
	
	With MPI_Comm_dup you can make an exact duplicate of a communicator. This may seem pointless, but it is actually very useful for the design of software libraries. 
	Image that you have a code:

	MPI_Isend(...); MPI_Irecv(...);
	// library call
	MPI_Waitall(...);
	and suppose that the library has receive calls. Now it is possible that the receive in the library inadvertently catches the message that was sent in the outer environment.

SPLITTING COMMUNICATORS

	Splitting a communicator into multiple disjoint communicators can be done with MPI_Comm_split And all processes in the old communicator with 
	the same colour wind up in a new communicator together. The old communicator still exists, so processes now have two different contexts in which to communicate.
	The ranking of processes in the new communicator is determined by a `key' value. Most of the time, there is no reason to use 
	a relative ranking that is different from the global ranking, so the MPI_Comm_rank value of the global communicator is a good choice.

GROUPS

	The most general mechanism is based on groups: you can extract the group from a communicator, combine different groups, and form a new communicator from the resulting group.

INTRA-COMMUNICATORS

	Intra-communicator : a collection of processes that can send messages to each other and engage in collective communication operations
	Composed of:
		group: ordered collection of processes, with each process assigned a unique rank
		context: system-defined object that uniquely identifies a communicators
		attributes: toplogy

INTER-COMMUNICATORS

	Inter-communicator: are used for sending messages between	processes belonging to disjoint intra-communicators. 
	If two disjoint communicators exist, it may be necessary to communicate between them. This can of course be done by creating a new communicator that overlaps them, 
	but this would be complicated: since the `inter' communication happens in the overlap communicator, you have to translate its ordering into those of the two worker communicators. 
	It would be easier to express messages directly in terms of those communicators, and this can be done with `inter-communicators'.

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

VIRTUAL TOPOLOGIES

	It is possible to associate additional information (beyond group and context) with a communicator.
	Topology is one of the attributes for communicator:
		-In MPI, a topology is a mechanism for associating different addressing schemes with the processes belonging to a group.
		-MPI topology is a virtual topology: there is no simple relation between the process structure and actual underlying physical structure of the parallel system.
		-Two main topology types: Cartesian (or grid) and graphs. Graphs are the more general case. 

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

MPID_REQUESTS

	MPI Requests are handles to MPID_Request structures. These are used for most communication operations to provide a uniform way in which to
	define pending operations.  As such, they contain many fields that are only used by some operations (logically, an MPID_Request is a union type).
	There are several kinds of requests.  They are:
		Send, Receive, RMA, User, Persistent

	Also, requests that are used internally within blocking MPI routines (only Send and Receive requests) do not require references to
	(or increments of the reference counts) communicators or datatypes. Thus, freeing these requests also does not require testing or 
	decrementing these fields.

	A posted (unmatched) receive queue entry needs only:
		match info
		buffer info (address, count, datatype)
		if nonblocking, communicator (used for finding error handler)
		if nonblocking, cancelled state

	Once matched, a receive queue entry also needs
		actual match info
	    message type (eager, rndv, eager-sync)
	    completion state (is all data available)

	An unexpected message (in the unexpected receive queue) needs only:
	    match info
	    message type (eager, rndv, eager-sync)
	    if (eager, eager-sync), data completion state (is all data available?)

	A send request requires only
	    message type (eager, rndv, eager-sync)
	    completion state (has all data been sent?)
	    canceled state
	    if nonblocking, communicator (used for finding error handler)
	    if the initial envelope is still pending (e.g., could not write yet), match info
	    if the data is still pending (rndv or would not send eager), buffer info (address, count, datatype)



//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

MPIDI ProcessGroups

	typedef struct MPIDI_PG
	{
	    MPIU_OBJECT_HEADER;  
	    	(adds handle and ref_count fields)
	    
	    struct MPIDI_PG * next; 
	    	(Next pointer used to maintain a list of all process groups known to this process )
	    
	    int size;	
	    	(Number of processes in the process group)

	    struct MPIDI_VC * vct;
	        (VC table.  At present this is a pointer to an array of VC structures. Someday we may want make this a pointer to an array
	        of VC references.  Thus, it is important to use MPIDI_PG_Get_vc() instead of directly referencing this field.)

	    void * id;
		    (Pointer to the process group ID.  The actual ID is defined and allocated by the process group.)

	    int finalize;
	        (Flag to mark a procress group which is finalizing. This means thay the VCs for this process group are closing, normally becuase
	        MPI_Finalize was called.) 


	    Connection information needed to access processes in this process group and to share the data with other processes.  
	    
	    void *connData;
	    	(pointer to an array of process group info)

	    int  (*getConnInfo)( int rank, char *buf, int bufsize, struct MPIDI_PG *self);
			(function to store into buf the connection information for rank in this process group)

	    int  (*connInfoToString)( char **buf_p, int *size, struct MPIDI_PG *self);
			(return in buf_p a string containing the info needed to support getConnInfo that can then be sent to another process to 
			recreate the connection information)

	    int  (*connInfoFromString)( const char *buf,  struct MPIDI_PG *self);
			(setup the information needed to implement getConnInfo)

	    int  (*freeConnInfo)( struct MPIDI_PG *self);
			(free any storage or resources associated with the connection information.)

	}

	  
	typedef struct MPIDI_Process
	{
	    MPIDI_PG_t * my_pg;
	    int my_pg_rank;
	}




//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

VIRTUAL CONNECTION

	In MPICH, connections are described by virtual connections. In many cases, these connections are not established until communication along
	that connection is required (for a more scalable approach). These virtual connections are associated with a single process group. Virtual connections
	are identified by their process group and a rank in that process group. 

	For each of their process groups, MPI communicators contain a virtual connection reference table (VCRT), which is basically a structure that contains
	an int number of virtual connections and a pointer to an array of virtual connections, indexed by rank in that communicator. There is also a reference
	count, allowing MPI_Comm_dup to make a shallow copy (refer to the VCRT rather than duplicating it). If a new communicator is not a dup, then a new
	VCRT is created.

	MPIDI_VC is a single virtual connection while a MPIDI_VCRT is a table of connections. Tables and connections are each reference counted; tables are used 
	within communicators, and connections are used during communication.

  	The virtual connection table contains the information used to contact a particular process, indexed by the rank relative to this communicator.

	To instantiate a virtual connection, the implementation can use the "KVS" (Key-value space) that the process manager associates with this process group
	to retreive connection information. For example, in the CH3 implementation of the ADI interface, each process inserts in the KVS space a "business card"
	that contains informaton (often a IP address and port for socket communication). Because the calling process is a member of the same process group,
	it can access this information with PMI_KVS_Get.

	int MPIDI_CH3_Get_business_card(int myRank, char *value, int length);

	vc - virtual connection to send the message over
	iov - a vector of a structure contains a buffer pointer and length
	iov_n - number of elements in the vector
 
	typedef struct MPIDI_VC
	{
	 	MPIU_Object fields.  MPIDI_VC_t objects are not allocated using the MPIU_Object system, but we do use the associated reference counting routines.  
	    The handle value is required when debugging objects (the handle kind is used in reporting on changes to the object).
	    
	    MPIU_OBJECT_HEADER; 
	    	(adds handle and ref_count fields)
	    
	    MPIDI_VC_State_t state;
	    	(state of the VC)

	    struct MPIDI_PG * pg;
	    	(Process group to which this VC belongs)

	    int pg_rank;
			(Rank of the process in that process group associated with this VC)

	    int lpid;
			(Local process ID)
	    
	    MPID_Node_id_t node_id;
	    	(The node id of this process, used for topologically aware collectives.)

	    int port_name_tag; 
	    	(added to handle dynamic process mgmt)
	    
	    MPID_Seqnum_t seqnum_send;
	    	(Sequence number of the next packet to be sent)

		int (* rndvSend_fn)( ... );
			(rendezvous function pointer, called to send a rendevous message or when one is matched)

	    int eager_max_msg_sz;
	    	(eager message threshold)

	}


	MPIDI_VCRT - virtual connection reference table

	handle - this element is not used, but exists so that we may use the MPIU_Object routines for reference counting
	ref_count - number of references to this table
	vcr_table - array of virtual connection references

	typedef struct MPIDI_VCRT
	{
	    MPIU_OBJECT_HEADER; (adds handle and ref_count fields)
	    int size;	(Number of entries in the table)
	    MPIDI_VC_t * vcr_table[1];	(array of virtual connection references)
	}


//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

CONTEXT ID

MPI evelope contains source, tag, communicator. The communicator is a logical construct indicating a particular communication context. In MPICH, this context is 
implemented via an additional tag field known as the context id.
The MPICH context ID is a 16-bit integer field that is structured as follows:

		V W W W W W W X X X X X L Y Y Z

	Dynamic Process Context Bit(V)
		If 0, this is a tradional context ID from the context ID mask. If 1, this is a context ID used for dynamic processes.
	Mask Word Index (W)
		This is the index into the context ID mask.
	Bit Index(X)
		This is which bit index within the mask that this ID refers to.
	Is Localcomm(L)
		Set to 1 if this communicator is a local communicator for an intercommunicator. 
	Sub-communicator Type(Y)
		Used to distinguish a "parent" communicaotr from any "child communicators" (Used for SMP-aware collective operations).
	Context Type Suffix(Z)
		Used to indicate different communication contexts within a communicator (point-to-point messages versus collective messages).

All members of a communicator use the same context ID for that communicator, but a context ID is not a globally unique ID. 
The communicator's group information combined with the context ID constitute a unique ID. 

The context ID mask is a bit vector used to keep track of which context IDs have been allocated. Currently it is an array of
64 32-bit unsigned ints for a total of 2048 bits. 
	
		IDs are allocated from the lowest available mask integer index but the highest available bit index within that integer.

MPI_Comm_create
	Just call MPIR_Get_contextid(old_comm_ptr).
MPI_Comm_split
	Call MPIR_Get_contextid(comm_ptr). This ID is the same across all the disjoint communicators that are created. 
	That is, if MPI_Comm_split is called such that three new communicators are created, the context ID will be the 
	same in all three communicators (although the groups will obviously be different between communicators).
MPI_Comm_dup
	This calls MPIR_Comm_copy which in turn calls MPIR_Get_contextid over the source communicator. This new 
	context ID is used for the duplicate communicator.

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

LOCAL AND GLOBAL MPI PROCESS IDS

	In order to indetify other processes in an MPI job, each MPI process maintains a "local" and a "global" process id.
	These process IDs are not related to any process ids managed by the underlying OS. Instead, they are used to manage
	the relationship between ranks in groups (and communicators) and processes.

	Local pids are used for all group operations and for identifying a particular remote process from a process. Different
	processes may use different local pids to refer to the same remote process; local processes only have meaning to the 
	process that is using them (ie. why they are called local).

	We assume that when processes are created, they are described by two numbers: (process-group-id)(rank-in-group). 
	Get the gpid corresponding to the rank of a process in a communicator. The routine MPID_GPID_GetAllInComm provides a single
	routine to efficiently access of of the gpids in a single communicator. 

	The local pids for the processes in MPI_COMM_WORLD have the same values as the ranks of those processes.

//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

PMI = Process Management Interface

	
        MPIDI_CH3I_VC_post_connect(vc) -> MPIDI_CH3I_VC_post_sockconnect(MPIDI_VC_t * vc)
        	Form a new connection 

        MPIDI_PG_GetConnString( vc->pg, vc->pg_rank, val, sizeof(val));
	       	This is an independent (not collective) call that returns the connection string for the process identified by a process 
	    	group and a rank in that process group. This may use PMI_KVS_Get for processes in the same MPI_COMM_WORLD

 			
 		getConnInfoKVS( int rank, char *buf, int bufsize, MPIDI_PG_t *pg )
			Function to store into buf the connection information for rank in this process group	
			Format of the process group description that is created and used by the connTo/FromString routines is (All items are strings, terminated by null):

			   process group id string
			   sizeof process group (as string)
			   conninfo for rank 0
			   conninfo for rank 1
			   ...

			   The "conninfo for rank 0" etc. for the original (MPI_COMM_WORLD) process group are stored in the PMI_KVS space with the keys p<rank>-businesscard . 


		PMI_KVS_Get( const char kvsname[], const char key[], char value[], int length) 
			Returns the key and connection information

	
		GetResponse( const char request[], const char expectedCmd[], int checkRc )

			This function is used to request information from the server and check that the response uses the expected command name.  On a successful
			return from this routine, additional PMIU_getval calls may be used to access information about the returned value.
 
 		
 		PMIU_getval( const char *keystr, char *valstr, int vallen )
	
	To instantiate a virtual connection, the implementation can use the "KVS" (Key-value space) that the process manager associates with this process group 
	to retreive connection information. In the CH3 implementation of the ADI interface, each process inserts into the KVS space a "business card" 
	that contains connection information (often a IP address and port for socket communication; it could be a shared memory segment id for shared-memory communication). 
	Because the calling process is a member of the same process group, it can access this information with PMI_KVS_Get.

	All interactions with external processes and resource managers, as well as the exchange of any information requried to contact other processes in the same parallel
	job, take place through the process management interface, or PMI. There are 4 separate sets of functionalities:
		1. Creating, connecting with, and exiting parallel jobs
		2. Accessing information about the parallel job or the node on which a process in running
		3. Exchanging information used to connect processes together
		4. Exchanging information related to the MPI Name publishing interface

	There are 3 groupings of processes that are important in understanding the process management interface:
		1. Process - An MPI process; usually an OS process
		2. Job - A collection of processes managed together by a process manager that understands parallel applications. A job contains all of the processes in a single
				 MPI_COMM_WORLD and no more.
		3. Connected Jobs - Collection of jobs that have established a connection through the use of PMI__Job_Connect


	Proceses in a parallel job often need a way to exchange information, such as how to establish a connection to a remote process. These routines establish a single 
	"key-value" space associated with each MPI_COMM_WORLD. One of these routines accepts a jobId; processes that are connected may read the "key-value" space of 
	another MPI_COMM_WORLD.

	Process Mangement comprises 3 primary components:
		1. The parallel programming library (MPICH, MVAPICH2, Intel-MPI)
		2. The PMI library (Simple PMI, SLURM PMI, SMPD MPI, BG/L MPI)
		3. The process manager (Hydra, MPD, SLURM, SMPD) - a logically centralized process (but often a distributed set of process in practice) that manages:
			a. process launching, providing the environmental information to each process
			b. information exchange between processes in a parallel application (setting yp communication channels)

	The PMI library provides the PMI API. The implementation of the PMI library, however, might depend on the system itself. While the PMI library can be implemented
	in any way that the particular implementation prefers, in both PMI-1 and PMI-2, there is a pre-defined "wire protocol" where data is exchanged through the socket interface.
	The advantage of using this protocol is that any application using the PMI API with the predefined wire protocol is compatible with any PMI process manager that 
	accepts the wire protocol. The PMI API and the PMI wire protocol are separaate entities; an implementation may choose to implement both or just one.

	Processes of a parallel application need to communicate with each other. Establishing this communication typically
	requires publishing a contact address, which may be ann IP address, a remotely accessible memory segment, or any other 
	interconnect-specific identifier. 

	In the case of MPI, PMI deals with aspects such as providing each MPI process with information about itself (such as its rank)
	as well as about the other processes in the application (ex: size of MPI_COMM_WORLD). Each PMI process manager that launches parallel
	applications is expected to maintain a database of all information. PMI defines a portable interface that allows the MPI process 
	to interact with the process manager by adding information to the database ("put" operations) and query information about
	other processes ("get" operations). The PMI functions are translated into the appropiate wire protocol by the PMI provider
	library and exchanged with the process manager. Most of the database is exchanged using "key-value" pairs.

	MPI processes cannot query key-value database information from the process manager in PMI-1. PMI-2 introduces the concept of 
	job attributes, which are predefined keys provided by the process manager. Using such keys, the process manager can pass system-specific 
	information to the MPI processes; this is, these keys are added into the key-value database directly by the process manager, 
	allowing each MPI process to get information about the layout of all MPI processes in a single operation.

*/













