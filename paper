Communicators provide a safe communication context for message passing, and are particularly useful in performing collectives over a subset of ranks. In this work, we specifically focus on intra-communicators due to their popularity and simplicity compared to inter-communicators, which is a topic that could be furthered studied in future work. Communicator support is absolutely essential in performing collectives in the network, yet few implementations of collective offload include or discuss it. It is much easier to just assume that the only communicator is MPI\_COMM\_WORLD, meaning the number of ranks involved in any collective operation is the same throughout a program. However, most MPI programs involve multiple communicators. The reason for a lack of research on multiple communicators in the network up to this point is that the combination of the MPI standard and current implementations of communicators do not force context id to be unique integers. Communicators are uniquely defined by the combination of a context id and a process group. Whenever messages are passed, context ids are passed within the message envelope and allow the receiver to identify the communicator that the message is a part of. However, the context ids are not necessary unique. For example, in an MPI\_Comm\_Split operation, an existing communicator such as MPI\_COMM\_WORLD is divided up into smaller communicators. Such an operation is common in creating a set row-wise or column-wise communicators when using a matrix of ranks. Even though each resulting communicator has its own mutually exclusive group of processors, the context ids of each communicator will be the same. In a regular processor with enough memory, it is trivial to receive a message, check its context id, and verify that the sender and receiver are both in the same communicator. However on an FPGA in the network with limited memory, it is not feasible to store the context ids and their corresponding process groups for all the communicators that are created during runtime, especially as the network size and process groups grow. 

We found that by enforcing context ids to be unique integers, we can design a data structure that can help speed up collective offload across multiple communicators while requiring minimal FPGA memory. In any intra-communicator, it is possible to know ahead of time which other ranks a given rank will be communicating with for any collective operation. For example, in an MPI\_Reduce operation of on short array, a binomial tree algorithm is used. With this tree structure, we know that rank 0 will be communicating with ranks 1, 2, 4, 8, and so on. In MPI\_Allgather on a short array, a recursive doubling algorithm is used, so we know that rank 1 will be communicating with ranks 0,3,5,7, and so on. By examining the MPICH-3.2 implementations of some of the most popular MPI collectives such as reduce, allreduce, scatter, broadcast, gather, and allgather, we found that all of these operations utilize a small set of algorithms. It is important to note that we did not write new algorithms for performing collectives, but rather found a method to efficiently implement the collective algorithms that are already used in MPICH. 

In the following table, we show the algorithms that are used for each collective operation in MPICH-3.2. We see from this table that there are 3 main algorithms used: binomial tree, recursive halving, and recursive doubling. The ring algorithm is trivial and was included in our implementation, but it is not discussed in this paper. In recursive doubling and recursive halving, the same ranks communicate with each other just in different orders. In binomial trees, the other ranks that a given ranks communicates with is just a subset of the ranks that it communicates with in the recursive halving and doubling algorithms. This means that whenever a communicator is created, we can determine for each rank all the other ranks that the given rank will be communicating with for collectives. We can then only store information on these ranks on the FPGA, since we will not be communicating with any other ranks during a collective. This limits the number of ranks that each FPGA must store data for to the log of the number of ranks in MPI\_COMM\_WORLD.


		Short message		Long Message
Reduce		binomial tree		recursive halving and recursive doubling
Allreduce	recursive doubling	recursive halving and recursive doubling
Bcast		Binomial Tree		Binomial Tree and recursive doubling or ring
Scatter		Binomial Tree		Binomial Tree
Gather		Binomial Tree		Binomial Tree
Allgather	Recursive Doubling	Ring


To effectively store this information, we created a “Communicator Table” with an entry for each communicator. Each entry stores the address of all the other ranks that a given rank will communicate with for all collectives. For example suppose that there are 16 ranks in a network. In the FPGA attached to the node that contains rank 0, its communicator table entry for MPI\_COMM\_WORLD would contain the physical addresses of ranks 1, 2, 4, and 8. We then use this table entry to perform all of the forwarding and multicasting required for each collective studied. For example, in a short broadcast, the FPGA belonging to rank 0 would multicast the message to ranks 1, 2, 4, and 8. 
