Communicators provide a safe communication context for message passing, and are particularly useful in performing collectives over a subset of ranks. In this work, we specifically focus on intra-communicators due to their popularity and simplicity compared to inter-communicators, which is a topic that could be furthered studied in future work. Communicator support is absolutely essential in performing collectives in the network, yet few implementations of collective offload include or discuss it. It is much easier to just assume that the only communicator is MPI\_COMM\_WORLD, meaning the number of ranks involved in any collective operation is the same throughout a program. However, most MPI programs involve multiple communicators. The reason for a lack of research on multiple communicators in the network up to this point is that the combination of the MPI standard and current implementations of communicators do not force context id to be unique integers. Communicators are uniquely defined by the combination of a context id and a process group. Whenever messages are passed, context ids are passed within the message envelope and allow the receiver to identify the communicator that the message is a part of. However, the context ids are not necessary unique. For example, in an MPI\_Comm\_Split operation, an existing communicator such as MPI\_COMM\_WORLD is divided up into smaller communicators. Such an operation is common in creating a set row-wise or column-wise communicators when using a matrix of ranks. Even though each resulting communicator has its own mutually exclusive group of processors, the context ids of each communicator will be the same. In a regular processor with enough memory, it is trivial to receive a message, check its context id, and verify that the sender and receiver are both in the same communicator. However on an FPGA in the network with limited memory, it is not feasible to store the context ids and their corresponding process groups for all the communicators that are created during runtime. 

We found that by enforcing context ids to be unique integers, we can design a data structure that can help speed up collective offload across multiple communicators while requiring minimal memory. In any intra-communicator, it is possible to know ahead of time which other ranks a given rank will be communicating with during a collective operation. For example, in an MPI\_Reduce operation of a short array, a binomial tree algorithm is used. With this tree structure, we know that rank 0 will be communicating with ranks 1, 2, 4, 8, and so on. In MPI\_Allgather on a short array, a recursive doubling algorithm is used, so we know that rank 1 will be communicating with ranks 0,3,5,7, and so on. By examining the MPICH-3.2 implementations of some of the most popular MPI collectives such as reduce, allreduce, scatter, broadcast, gather, and allgather, we found that all of these operations utilize a small set of algorithms. 

In the following table, we show the algorithms that are used for each collective operation in MPICH-3.2. We see from this table that there are 3 main algorithms used: binomial tree, recursive halving, and recursive doubling. For recursive doubling and recursive halving, the same ranks communicate with each other just in different orders. In binomial trees, the other ranks that a given ranks communicates with is just a subset of the ranks that it communicates with in the recursive halving and doubling algorithms. This means that whenever a communicator is created, we can determine for each ranks all the other ranks that the given rank will be communicating with for collectives.    

		Short message		Long Message
Reduce		binomial tree		recursive halving and recursive doubling
Allreduce	recursive doubling	recursive halving and recursive doubling
Bcast		Binomial Tree		Binomial Tree and recursive doubling or ring
Scatter		Binomial Tree		Binomial Tree
Gather		Binomial Tree		Binomial Tree
Allgather	Recursive Doubling	Ring

We can then only store information on these ranks on the FPGA, since we will not be communicating with any other ranks during a collective. This limits the number of ranks that each FPGA must store data for to the log of the number of ranks in MPI\_COMM\_WORLD.
